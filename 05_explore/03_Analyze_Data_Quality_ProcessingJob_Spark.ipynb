{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# Analyze Data Quality with SageMaker Processing Jobs and Spark\n",
    "\n",
    "Typically a machine learning (ML) process consists of few steps. First, gathering data with various ETL jobs, then pre-processing the data, featurizing the dataset by incorporating standard techniques or prior knowledge, and finally training an ML model using an algorithm.\n",
    "\n",
    "Often, distributed data processing frameworks such as Spark are used to process and analyze data sets in order to detect data quality issues and prepare them for model training.  \n",
    "\n",
    "In this notebook we'll use Amazon SageMaker Processing with a library called [**Deequ**](https://github.com/awslabs/deequ), and leverage the power of Spark with a managed SageMaker Processing Job to run our data processing workloads.\n",
    "\n",
    "Here is a great blog post on Deequ for more information:  https://aws.amazon.com/blogs/big-data/test-data-quality-at-scale-with-deequ/\n",
    "\n",
    "![Deequ](img/deequ.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](img/processing.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Amazon Customer Reviews Dataset\n",
    "\n",
    "https://s3.amazonaws.com/amazon-reviews-pds/readme.html\n",
    "\n",
    "### Dataset Columns:\n",
    "\n",
    "- `marketplace`: 2-letter country code (in this case all \"US\").\n",
    "- `customer_id`: Random identifier that can be used to aggregate reviews written by a single author.\n",
    "- `review_id`: A unique ID for the review.\n",
    "- `product_id`: The Amazon Standard Identification Number (ASIN).  `http://www.amazon.com/dp/<ASIN>` links to the product's detail page.\n",
    "- `product_parent`: The parent of that ASIN.  Multiple ASINs (color or format variations of the same product) can roll up into a single parent.\n",
    "- `product_title`: Title description of the product.\n",
    "- `product_category`: Broad product category that can be used to group reviews (in this case digital videos).\n",
    "- `star_rating`: The review's rating (1 to 5 stars).\n",
    "- `helpful_votes`: Number of helpful votes for the review.\n",
    "- `total_votes`: Number of total votes the review received.\n",
    "- `vine`: Was the review written as part of the [Vine](https://www.amazon.com/gp/vine/help) program?\n",
    "- `verified_purchase`: Was the review from a verified purchase?\n",
    "- `review_headline`: The title of the review itself.\n",
    "- `review_body`: The text of the review.\n",
    "- `review_date`: The date the review was written."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sagemaker\n",
    "\n",
    "sagemaker_session = sagemaker.Session()\n",
    "role = sagemaker.get_execution_role()\n",
    "bucket = sagemaker_session.default_bucket()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build a Spark Docker Image to Run the Processing Job"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An example Spark container is included in the `./container` directory of this example. The container handles the bootstrapping of all Spark configuration, and serves as a wrapper around the `spark-submit` CLI. At a high level the container provides:\n",
    "* A set of default Spark/YARN/Hadoop configurations\n",
    "* A bootstrapping script for configuring and starting up Spark master/worker nodes\n",
    "* A wrapper around the `spark-submit` CLI to submit a Spark application\n",
    "\n",
    "\n",
    "After the container build and push process is complete, use the Amazon SageMaker Python SDK to submit a managed, distributed Spark application that performs our dataset preprocessing.\n",
    "\n",
    "Build the example Spark container."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34mFROM\u001b[39;49;00m \u001b[33mopenjdk:8-jre-slim\u001b[39;49;00m\r\n",
      "\r\n",
      "\u001b[34mRUN\u001b[39;49;00m apt-get update\r\n",
      "\u001b[34mRUN\u001b[39;49;00m apt-get install -y curl unzip python3 python3-setuptools python3-pip python-dev python3-dev python-psutil\r\n",
      "\u001b[34mRUN\u001b[39;49;00m pip3 install py4j \u001b[31mpsutil\u001b[39;49;00m==\u001b[34m5\u001b[39;49;00m.6.5 \u001b[31mnumpy\u001b[39;49;00m==\u001b[34m1\u001b[39;49;00m.17.4\r\n",
      "\u001b[34mRUN\u001b[39;49;00m apt-get clean\r\n",
      "\u001b[34mRUN\u001b[39;49;00m rm -rf /var/lib/apt/lists/*\r\n",
      "\r\n",
      "\u001b[37m# http://blog.stuart.axelbrooke.com/python-3-on-spark-return-of-the-pythonhashseed\u001b[39;49;00m\r\n",
      "\u001b[34mENV\u001b[39;49;00m PYTHONHASHSEED \u001b[34m0\u001b[39;49;00m\r\n",
      "\u001b[34mENV\u001b[39;49;00m PYTHONIOENCODING UTF-8\r\n",
      "\u001b[34mENV\u001b[39;49;00m PIP_DISABLE_PIP_VERSION_CHECK \u001b[34m1\u001b[39;49;00m\r\n",
      "\r\n",
      "\u001b[37m# Install Hadoop\u001b[39;49;00m\r\n",
      "\u001b[34mENV\u001b[39;49;00m HADOOP_VERSION \u001b[34m3\u001b[39;49;00m.2.1\r\n",
      "\u001b[34mENV\u001b[39;49;00m HADOOP_HOME /usr/hadoop-\u001b[31m$HADOOP_VERSION\u001b[39;49;00m\r\n",
      "\u001b[34mENV\u001b[39;49;00m \u001b[31mHADOOP_CONF_DIR\u001b[39;49;00m=\u001b[31m$HADOOP_HOME\u001b[39;49;00m/etc/hadoop\r\n",
      "\u001b[34mENV\u001b[39;49;00m PATH \u001b[31m$PATH\u001b[39;49;00m:\u001b[31m$HADOOP_HOME\u001b[39;49;00m/bin\r\n",
      "\u001b[34mRUN\u001b[39;49;00m curl -sL --retry \u001b[34m3\u001b[39;49;00m \u001b[33m\\\u001b[39;49;00m\r\n",
      "  \u001b[33m\"\u001b[39;49;00m\u001b[33mhttp://archive.apache.org/dist/hadoop/common/hadoop-\u001b[39;49;00m\u001b[31m$HADOOP_VERSION\u001b[39;49;00m\u001b[33m/hadoop-\u001b[39;49;00m\u001b[31m$HADOOP_VERSION\u001b[39;49;00m\u001b[33m.tar.gz\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m \u001b[33m\\\u001b[39;49;00m\r\n",
      "  | gunzip \u001b[33m\\\u001b[39;49;00m\r\n",
      "  | tar -x -C /usr/ \u001b[33m\\\u001b[39;49;00m\r\n",
      " && rm -rf \u001b[31m$HADOOP_HOME\u001b[39;49;00m/share/doc \u001b[33m\\\u001b[39;49;00m\r\n",
      " && chown -R root:root \u001b[31m$HADOOP_HOME\u001b[39;49;00m\r\n",
      "\r\n",
      "\u001b[37m# Install Spark\u001b[39;49;00m\r\n",
      "\u001b[34mENV\u001b[39;49;00m SPARK_VERSION \u001b[34m2\u001b[39;49;00m.4.6\r\n",
      "\u001b[34mENV\u001b[39;49;00m SPARK_PACKAGE spark-\u001b[33m${\u001b[39;49;00m\u001b[31mSPARK_VERSION\u001b[39;49;00m\u001b[33m}\u001b[39;49;00m-bin-without-hadoop\r\n",
      "\u001b[34mENV\u001b[39;49;00m SPARK_HOME /usr/spark-\u001b[33m${\u001b[39;49;00m\u001b[31mSPARK_VERSION\u001b[39;49;00m\u001b[33m}\u001b[39;49;00m\r\n",
      "\u001b[34mENV\u001b[39;49;00m \u001b[31mSPARK_DIST_CLASSPATH\u001b[39;49;00m=\u001b[33m\"\u001b[39;49;00m\u001b[31m$HADOOP_HOME\u001b[39;49;00m\u001b[33m/etc/hadoop/*:\u001b[39;49;00m\u001b[31m$HADOOP_HOME\u001b[39;49;00m\u001b[33m/share/hadoop/common/lib/*:\u001b[39;49;00m\u001b[31m$HADOOP_HOME\u001b[39;49;00m\u001b[33m/share/hadoop/common/*:\u001b[39;49;00m\u001b[31m$HADOOP_HOME\u001b[39;49;00m\u001b[33m/share/hadoop/hdfs/*:\u001b[39;49;00m\u001b[31m$HADOOP_HOME\u001b[39;49;00m\u001b[33m/share/hadoop/hdfs/lib/*:\u001b[39;49;00m\u001b[31m$HADOOP_HOME\u001b[39;49;00m\u001b[33m/share/hadoop/hdfs/*:\u001b[39;49;00m\u001b[31m$HADOOP_HOME\u001b[39;49;00m\u001b[33m/share/hadoop/yarn/lib/*:\u001b[39;49;00m\u001b[31m$HADOOP_HOME\u001b[39;49;00m\u001b[33m/share/hadoop/yarn/*:\u001b[39;49;00m\u001b[31m$HADOOP_HOME\u001b[39;49;00m\u001b[33m/share/hadoop/mapreduce/lib/*:\u001b[39;49;00m\u001b[31m$HADOOP_HOME\u001b[39;49;00m\u001b[33m/share/hadoop/mapreduce/*:\u001b[39;49;00m\u001b[31m$HADOOP_HOME\u001b[39;49;00m\u001b[33m/share/hadoop/tools/lib/*\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m\r\n",
      "\u001b[34mENV\u001b[39;49;00m PATH \u001b[31m$PATH\u001b[39;49;00m:\u001b[33m${\u001b[39;49;00m\u001b[31mSPARK_HOME\u001b[39;49;00m\u001b[33m}\u001b[39;49;00m/bin\r\n",
      "\u001b[34mRUN\u001b[39;49;00m curl -sL --retry \u001b[34m3\u001b[39;49;00m \u001b[33m\\\u001b[39;49;00m\r\n",
      "  \u001b[33m\"\u001b[39;49;00m\u001b[33mhttps://archive.apache.org/dist/spark/spark-\u001b[39;49;00m\u001b[33m${\u001b[39;49;00m\u001b[31mSPARK_VERSION\u001b[39;49;00m\u001b[33m}\u001b[39;49;00m\u001b[33m/\u001b[39;49;00m\u001b[33m${\u001b[39;49;00m\u001b[31mSPARK_PACKAGE\u001b[39;49;00m\u001b[33m}\u001b[39;49;00m\u001b[33m.tgz\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m \u001b[33m\\\u001b[39;49;00m\r\n",
      "  | gunzip \u001b[33m\\\u001b[39;49;00m\r\n",
      "  | tar x -C /usr/ \u001b[33m\\\u001b[39;49;00m\r\n",
      " && mv /usr/\u001b[31m$SPARK_PACKAGE\u001b[39;49;00m \u001b[31m$SPARK_HOME\u001b[39;49;00m \u001b[33m\\\u001b[39;49;00m\r\n",
      " && chown -R root:root \u001b[31m$SPARK_HOME\u001b[39;49;00m\r\n",
      " \r\n",
      "\u001b[37m# Point Spark at proper python binary\u001b[39;49;00m\r\n",
      "\u001b[34mENV\u001b[39;49;00m \u001b[31mPYSPARK_PYTHON\u001b[39;49;00m=/usr/bin/python3\r\n",
      "\r\n",
      "\u001b[37m# Setup Spark/Yarn/HDFS user as root\u001b[39;49;00m\r\n",
      "\u001b[34mENV\u001b[39;49;00m \u001b[31mPATH\u001b[39;49;00m=\u001b[33m\"\u001b[39;49;00m\u001b[33m/usr/bin:/opt/program:\u001b[39;49;00m\u001b[33m${\u001b[39;49;00m\u001b[31mPATH\u001b[39;49;00m\u001b[33m}\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m\r\n",
      "\u001b[34mENV\u001b[39;49;00m \u001b[31mYARN_RESOURCEMANAGER_USER\u001b[39;49;00m=\u001b[33m\"root\"\u001b[39;49;00m\r\n",
      "\u001b[34mENV\u001b[39;49;00m \u001b[31mYARN_NODEMANAGER_USER\u001b[39;49;00m=\u001b[33m\"root\"\u001b[39;49;00m\r\n",
      "\u001b[34mENV\u001b[39;49;00m \u001b[31mHDFS_NAMENODE_USER\u001b[39;49;00m=\u001b[33m\"root\"\u001b[39;49;00m\r\n",
      "\u001b[34mENV\u001b[39;49;00m \u001b[31mHDFS_DATANODE_USER\u001b[39;49;00m=\u001b[33m\"root\"\u001b[39;49;00m\r\n",
      "\u001b[34mENV\u001b[39;49;00m \u001b[31mHDFS_SECONDARYNAMENODE_USER\u001b[39;49;00m=\u001b[33m\"root\"\u001b[39;49;00m\r\n",
      "\r\n",
      "\u001b[37m# Set up bootstrapping program and Spark configuration\u001b[39;49;00m\r\n",
      "\u001b[34mCOPY\u001b[39;49;00m program /opt/program\r\n",
      "\u001b[34mRUN\u001b[39;49;00m chmod +x /opt/program/submit\r\n",
      "\u001b[34mCOPY\u001b[39;49;00m hadoop-config /opt/hadoop-config\r\n",
      "\r\n",
      "\u001b[34mCOPY\u001b[39;49;00m jars /usr/jars\r\n",
      "\r\n",
      "\u001b[34mWORKDIR\u001b[39;49;00m\u001b[33m $SPARK_HOME\u001b[39;49;00m\r\n",
      "\r\n",
      "\u001b[37m# Install Transformers and TensorFlow\u001b[39;49;00m\r\n",
      "\u001b[37m#RUN pip3 install -q pip --upgrade\u001b[39;49;00m\r\n",
      "\u001b[37m#RUN pip3 install -q wrapt --upgrade --ignore-installed\u001b[39;49;00m\r\n",
      "\u001b[37m#RUN pip3 install -q transformers==2.8.0\u001b[39;49;00m\r\n",
      "\u001b[37m#RUN pip3 install -q tensorflow==2.1.0 --upgrade --ignore-installed\u001b[39;49;00m\r\n",
      "\r\n",
      "\u001b[34mENTRYPOINT\u001b[39;49;00m [\u001b[33m\"/opt/program/submit\"\u001b[39;49;00m]\r\n"
     ]
    }
   ],
   "source": [
    "!pygmentize container/Dockerfile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "docker_repo = 'amazon-reviews-spark-analyzer'\n",
    "docker_tag = 'latest'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sending build context to Docker daemon  4.441MB\n",
      "Step 1/33 : FROM openjdk:8-jre-slim\n",
      " ---> d2f9f3c77c25\n",
      "Step 2/33 : RUN apt-get update\n",
      " ---> Using cache\n",
      " ---> de849c6fc99d\n",
      "Step 3/33 : RUN apt-get install -y curl unzip python3 python3-setuptools python3-pip python-dev python3-dev python-psutil\n",
      " ---> Using cache\n",
      " ---> b573599ff428\n",
      "Step 4/33 : RUN pip3 install py4j psutil==5.6.5 numpy==1.17.4\n",
      " ---> Using cache\n",
      " ---> c301bde7719a\n",
      "Step 5/33 : RUN apt-get clean\n",
      " ---> Using cache\n",
      " ---> 7540ff0ab66c\n",
      "Step 6/33 : RUN rm -rf /var/lib/apt/lists/*\n",
      " ---> Using cache\n",
      " ---> a79c398a7313\n",
      "Step 7/33 : ENV PYTHONHASHSEED 0\n",
      " ---> Using cache\n",
      " ---> 069665758a56\n",
      "Step 8/33 : ENV PYTHONIOENCODING UTF-8\n",
      " ---> Using cache\n",
      " ---> bea202ee24bf\n",
      "Step 9/33 : ENV PIP_DISABLE_PIP_VERSION_CHECK 1\n",
      " ---> Using cache\n",
      " ---> 25d205e83039\n",
      "Step 10/33 : ENV HADOOP_VERSION 3.2.1\n",
      " ---> Using cache\n",
      " ---> a1c381898ac2\n",
      "Step 11/33 : ENV HADOOP_HOME /usr/hadoop-$HADOOP_VERSION\n",
      " ---> Using cache\n",
      " ---> 942dd8611a11\n",
      "Step 12/33 : ENV HADOOP_CONF_DIR=$HADOOP_HOME/etc/hadoop\n",
      " ---> Using cache\n",
      " ---> 1cd19fa379f6\n",
      "Step 13/33 : ENV PATH $PATH:$HADOOP_HOME/bin\n",
      " ---> Using cache\n",
      " ---> da34843d8117\n",
      "Step 14/33 : RUN curl -sL --retry 3   \"http://archive.apache.org/dist/hadoop/common/hadoop-$HADOOP_VERSION/hadoop-$HADOOP_VERSION.tar.gz\"   | gunzip   | tar -x -C /usr/  && rm -rf $HADOOP_HOME/share/doc  && chown -R root:root $HADOOP_HOME\n",
      " ---> Running in df7a07f29aad\n",
      "\u001b[91m\n",
      "gzip: stdin: not in gzip format\n",
      "\u001b[0m\u001b[91mtar: This does not look like a tar archive\n",
      "tar: Exiting with failure status due to previous errors\n",
      "\u001b[0mThe command '/bin/sh -c curl -sL --retry 3   \"http://archive.apache.org/dist/hadoop/common/hadoop-$HADOOP_VERSION/hadoop-$HADOOP_VERSION.tar.gz\"   | gunzip   | tar -x -C /usr/  && rm -rf $HADOOP_HOME/share/doc  && chown -R root:root $HADOOP_HOME' returned a non-zero code: 2\n"
     ]
    }
   ],
   "source": [
    "!docker build -t $docker_repo:$docker_tag -f container/Dockerfile ./container"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Check the Docker Image\n",
    "If the image did not build properly, re-run the cell above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[]\r\n",
      "Error: No such object: amazon-reviews-spark-analyzer:latest\r\n"
     ]
    }
   ],
   "source": [
    "!docker inspect $docker_repo:$docker_tag"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Push the Image to a Private Docker Repo (Amazon ECR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "032934710550.dkr.ecr.us-west-2.amazonaws.com/amazon-reviews-spark-analyzer:latest\n"
     ]
    }
   ],
   "source": [
    "import boto3\n",
    "account_id = boto3.client('sts').get_caller_identity().get('Account')\n",
    "region = boto3.session.Session().region_name\n",
    "\n",
    "image_uri = '{}.dkr.ecr.{}.amazonaws.com/{}:{}'.format(account_id, region, docker_repo, docker_tag)\n",
    "print(image_uri)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING! Using --password via the CLI is insecure. Use --password-stdin.\n",
      "WARNING! Your password will be stored unencrypted in /home/ec2-user/.docker/config.json.\n",
      "Configure a credential helper to remove this warning. See\n",
      "https://docs.docker.com/engine/reference/commandline/login/#credentials-store\n",
      "\n",
      "Login Succeeded\n"
     ]
    }
   ],
   "source": [
    "!$(aws ecr get-login --region $region --registry-ids $account_id --no-include-email)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "An error occurred (RepositoryNotFoundException) when calling the DescribeRepositories operation: The repository with name 'amazon-reviews-spark-analyzer' does not exist in the registry with id '032934710550'\n",
      "{\n",
      "    \"repository\": {\n",
      "        \"repositoryArn\": \"arn:aws:ecr:us-west-2:032934710550:repository/amazon-reviews-spark-analyzer\",\n",
      "        \"registryId\": \"032934710550\",\n",
      "        \"repositoryName\": \"amazon-reviews-spark-analyzer\",\n",
      "        \"repositoryUri\": \"032934710550.dkr.ecr.us-west-2.amazonaws.com/amazon-reviews-spark-analyzer\",\n",
      "        \"createdAt\": 1598123779.0,\n",
      "        \"imageTagMutability\": \"MUTABLE\",\n",
      "        \"imageScanningConfiguration\": {\n",
      "            \"scanOnPush\": false\n",
      "        },\n",
      "        \"encryptionConfiguration\": {\n",
      "            \"encryptionType\": \"AES256\"\n",
      "        }\n",
      "    }\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "!aws ecr describe-repositories --repository-names $docker_repo || aws ecr create-repository --repository-name $docker_repo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error response from daemon: No such image: amazon-reviews-spark-analyzer:latest\r\n"
     ]
    }
   ],
   "source": [
    "!docker tag $docker_repo:$docker_tag $image_uri"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The push refers to repository [032934710550.dkr.ecr.us-west-2.amazonaws.com/amazon-reviews-spark-analyzer]\r\n",
      "An image does not exist locally with the tag: 032934710550.dkr.ecr.us-west-2.amazonaws.com/amazon-reviews-spark-analyzer\r\n"
     ]
    }
   ],
   "source": [
    "!docker push $image_uri"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run the Analysis Job using a SageMaker Processing Job\n",
    "\n",
    "Next, use the Amazon SageMaker Python SDK to submit a processing job. Use the Spark container that was just built with our Spark script."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Review the Spark preprocessing script."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34mfrom\u001b[39;49;00m \u001b[04m\u001b[36m__future__\u001b[39;49;00m \u001b[34mimport\u001b[39;49;00m print_function\r\n",
      "\u001b[34mfrom\u001b[39;49;00m \u001b[04m\u001b[36m__future__\u001b[39;49;00m \u001b[34mimport\u001b[39;49;00m unicode_literals\r\n",
      "\r\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36mtime\u001b[39;49;00m\r\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36msys\u001b[39;49;00m\r\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36mos\u001b[39;49;00m\r\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36mshutil\u001b[39;49;00m\r\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36mcsv\u001b[39;49;00m\r\n",
      "\r\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36mpyspark\u001b[39;49;00m\r\n",
      "\u001b[34mfrom\u001b[39;49;00m \u001b[04m\u001b[36mpyspark\u001b[39;49;00m\u001b[04m\u001b[36m.\u001b[39;49;00m\u001b[04m\u001b[36msql\u001b[39;49;00m \u001b[34mimport\u001b[39;49;00m SparkSession\r\n",
      "\u001b[34mfrom\u001b[39;49;00m \u001b[04m\u001b[36mpyspark\u001b[39;49;00m\u001b[04m\u001b[36m.\u001b[39;49;00m\u001b[04m\u001b[36msql\u001b[39;49;00m\u001b[04m\u001b[36m.\u001b[39;49;00m\u001b[04m\u001b[36mfunctions\u001b[39;49;00m \u001b[34mimport\u001b[39;49;00m *\r\n",
      "\r\n",
      "\u001b[34mdef\u001b[39;49;00m \u001b[32mmain\u001b[39;49;00m():\r\n",
      "    args_iter = \u001b[36miter\u001b[39;49;00m(sys.argv[\u001b[34m1\u001b[39;49;00m:])\r\n",
      "    args = \u001b[36mdict\u001b[39;49;00m(\u001b[36mzip\u001b[39;49;00m(args_iter, args_iter))\r\n",
      "    \r\n",
      "    \u001b[37m# Retrieve the args and replace 's3://' with 's3a://' (used by Spark)\u001b[39;49;00m\r\n",
      "    s3_input_data = args[\u001b[33m'\u001b[39;49;00m\u001b[33ms3_input_data\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m].replace(\u001b[33m'\u001b[39;49;00m\u001b[33ms3://\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, \u001b[33m'\u001b[39;49;00m\u001b[33ms3a://\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)\r\n",
      "    \u001b[36mprint\u001b[39;49;00m(s3_input_data)\r\n",
      "    s3_output_analyze_data = args[\u001b[33m'\u001b[39;49;00m\u001b[33ms3_output_analyze_data\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m].replace(\u001b[33m'\u001b[39;49;00m\u001b[33ms3://\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, \u001b[33m'\u001b[39;49;00m\u001b[33ms3a://\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)\r\n",
      "    \u001b[36mprint\u001b[39;49;00m(s3_output_analyze_data)\r\n",
      "    \r\n",
      "    spark = SparkSession.builder \\\r\n",
      "        .appName(\u001b[33m\"\u001b[39;49;00m\u001b[33mAmazon_Reviews_Spark_Analyzer\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m) \\\r\n",
      "        .getOrCreate()\r\n",
      "\r\n",
      "    \u001b[37m# Invoke Main from preprocess-deequ.jar\u001b[39;49;00m\r\n",
      "    \u001b[36mgetattr\u001b[39;49;00m(spark._jvm.SparkAmazonReviewsAnalyzer, \u001b[33m\"\u001b[39;49;00m\u001b[33mrun\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m)(s3_input_data, s3_output_analyze_data)\r\n",
      "\r\n",
      "\u001b[34mif\u001b[39;49;00m \u001b[31m__name__\u001b[39;49;00m == \u001b[33m\"\u001b[39;49;00m\u001b[33m__main__\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m:\r\n",
      "    main()\r\n"
     ]
    }
   ],
   "source": [
    "!pygmentize preprocess-deequ.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36mcom.amazon.deequ.analyzers.runners.\u001b[39;49;00m{\u001b[04m\u001b[32mAnalysisRunner\u001b[39;49;00m, \u001b[04m\u001b[32mAnalyzerContext\u001b[39;49;00m}\r\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36mcom.amazon.deequ.analyzers.runners.AnalyzerContext.successMetricsAsDataFrame\u001b[39;49;00m\r\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36mcom.amazon.deequ.analyzers.\u001b[39;49;00m{\u001b[04m\u001b[32mCompliance\u001b[39;49;00m, \u001b[04m\u001b[32mCorrelation\u001b[39;49;00m, \u001b[04m\u001b[32mSize\u001b[39;49;00m, \u001b[04m\u001b[32mCompleteness\u001b[39;49;00m, \u001b[04m\u001b[32mMean\u001b[39;49;00m, \u001b[04m\u001b[32mApproxCountDistinct\u001b[39;49;00m}\r\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36mcom.amazon.deequ.\u001b[39;49;00m{\u001b[04m\u001b[32mVerificationSuite\u001b[39;49;00m, \u001b[04m\u001b[32mVerificationResult\u001b[39;49;00m}\r\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36mcom.amazon.deequ.VerificationResult.checkResultsAsDataFrame\u001b[39;49;00m\r\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36mcom.amazon.deequ.checks.\u001b[39;49;00m{\u001b[04m\u001b[32mCheck\u001b[39;49;00m, \u001b[04m\u001b[32mCheckLevel\u001b[39;49;00m}\r\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36mcom.amazon.deequ.suggestions.\u001b[39;49;00m{\u001b[04m\u001b[32mConstraintSuggestionRunner\u001b[39;49;00m, \u001b[04m\u001b[32mRules\u001b[39;49;00m}\r\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36morg.apache.spark.sql.SparkSession\u001b[39;49;00m\r\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36morg.apache.spark.sql.SaveMode\u001b[39;49;00m\r\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36morg.apache.spark.sql.types.\u001b[39;49;00m{\u001b[04m\u001b[32mStructType\u001b[39;49;00m, \u001b[04m\u001b[32mStructField\u001b[39;49;00m, \u001b[04m\u001b[32mStringType\u001b[39;49;00m, \u001b[04m\u001b[32mIntegerType\u001b[39;49;00m}\r\n",
      "\r\n",
      "\r\n",
      "\u001b[34mobject\u001b[39;49;00m \u001b[04m\u001b[32mSparkAmazonReviewsAnalyzer\u001b[39;49;00m {\r\n",
      "  \u001b[34mdef\u001b[39;49;00m run(s3InputData\u001b[34m:\u001b[39;49;00m \u001b[36mString\u001b[39;49;00m, s3OutputAnalyzeData\u001b[34m:\u001b[39;49;00m \u001b[36mString\u001b[39;49;00m)\u001b[34m:\u001b[39;49;00m \u001b[36mUnit\u001b[39;49;00m = {\r\n",
      "\r\n",
      "    \u001b[04m\u001b[32mSystem\u001b[39;49;00m.out.println(\u001b[33ms\"\u001b[39;49;00m\u001b[33ms3_input_data: \u001b[39;49;00m\u001b[33m${\u001b[39;49;00ms3InputData\u001b[33m}\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m)\r\n",
      "    \u001b[04m\u001b[32mSystem\u001b[39;49;00m.out.println(\u001b[33ms\"\u001b[39;49;00m\u001b[33ms3_output_analyze_data: \u001b[39;49;00m\u001b[33m${\u001b[39;49;00ms3OutputAnalyzeData\u001b[33m}\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m)\r\n",
      "      \r\n",
      "    \u001b[34mval\u001b[39;49;00m spark \u001b[34m=\u001b[39;49;00m \u001b[04m\u001b[32mSparkSession\u001b[39;49;00m\r\n",
      "      .builder\r\n",
      "      .appName(\u001b[33m\"SparkAmazonReviewsAnalyzer\"\u001b[39;49;00m)\r\n",
      "      .getOrCreate()\r\n",
      "    \r\n",
      "    \u001b[34mval\u001b[39;49;00m schema \u001b[34m=\u001b[39;49;00m \u001b[04m\u001b[32mStructType\u001b[39;49;00m(\u001b[04m\u001b[32mArray\u001b[39;49;00m(\r\n",
      "        \u001b[04m\u001b[32mStructField\u001b[39;49;00m(\u001b[33m\"marketplace\"\u001b[39;49;00m, \u001b[04m\u001b[32mStringType\u001b[39;49;00m, \u001b[34mtrue\u001b[39;49;00m),\r\n",
      "        \u001b[04m\u001b[32mStructField\u001b[39;49;00m(\u001b[33m\"customer_id\"\u001b[39;49;00m, \u001b[04m\u001b[32mStringType\u001b[39;49;00m, \u001b[34mtrue\u001b[39;49;00m),\r\n",
      "        \u001b[04m\u001b[32mStructField\u001b[39;49;00m(\u001b[33m\"review_id\"\u001b[39;49;00m, \u001b[04m\u001b[32mStringType\u001b[39;49;00m, \u001b[34mtrue\u001b[39;49;00m),\r\n",
      "        \u001b[04m\u001b[32mStructField\u001b[39;49;00m(\u001b[33m\"product_id\"\u001b[39;49;00m, \u001b[04m\u001b[32mStringType\u001b[39;49;00m, \u001b[34mtrue\u001b[39;49;00m),\r\n",
      "        \u001b[04m\u001b[32mStructField\u001b[39;49;00m(\u001b[33m\"product_parent\"\u001b[39;49;00m, \u001b[04m\u001b[32mStringType\u001b[39;49;00m, \u001b[34mtrue\u001b[39;49;00m),\r\n",
      "        \u001b[04m\u001b[32mStructField\u001b[39;49;00m(\u001b[33m\"product_title\"\u001b[39;49;00m, \u001b[04m\u001b[32mStringType\u001b[39;49;00m, \u001b[34mtrue\u001b[39;49;00m),\r\n",
      "        \u001b[04m\u001b[32mStructField\u001b[39;49;00m(\u001b[33m\"product_category\"\u001b[39;49;00m, \u001b[04m\u001b[32mStringType\u001b[39;49;00m, \u001b[34mtrue\u001b[39;49;00m),\r\n",
      "        \u001b[04m\u001b[32mStructField\u001b[39;49;00m(\u001b[33m\"star_rating\"\u001b[39;49;00m, \u001b[04m\u001b[32mIntegerType\u001b[39;49;00m, \u001b[34mtrue\u001b[39;49;00m),\r\n",
      "        \u001b[04m\u001b[32mStructField\u001b[39;49;00m(\u001b[33m\"helpful_votes\"\u001b[39;49;00m, \u001b[04m\u001b[32mIntegerType\u001b[39;49;00m, \u001b[34mtrue\u001b[39;49;00m),\r\n",
      "        \u001b[04m\u001b[32mStructField\u001b[39;49;00m(\u001b[33m\"total_votes\"\u001b[39;49;00m, \u001b[04m\u001b[32mIntegerType\u001b[39;49;00m, \u001b[34mtrue\u001b[39;49;00m),\r\n",
      "        \u001b[04m\u001b[32mStructField\u001b[39;49;00m(\u001b[33m\"vine\"\u001b[39;49;00m, \u001b[04m\u001b[32mStringType\u001b[39;49;00m, \u001b[34mtrue\u001b[39;49;00m),\r\n",
      "        \u001b[04m\u001b[32mStructField\u001b[39;49;00m(\u001b[33m\"verified_purchase\"\u001b[39;49;00m, \u001b[04m\u001b[32mStringType\u001b[39;49;00m, \u001b[34mtrue\u001b[39;49;00m),\r\n",
      "        \u001b[04m\u001b[32mStructField\u001b[39;49;00m(\u001b[33m\"review_headline\"\u001b[39;49;00m, \u001b[04m\u001b[32mStringType\u001b[39;49;00m, \u001b[34mtrue\u001b[39;49;00m),\r\n",
      "        \u001b[04m\u001b[32mStructField\u001b[39;49;00m(\u001b[33m\"review_body\"\u001b[39;49;00m, \u001b[04m\u001b[32mStringType\u001b[39;49;00m, \u001b[34mtrue\u001b[39;49;00m),\r\n",
      "        \u001b[04m\u001b[32mStructField\u001b[39;49;00m(\u001b[33m\"review_date\"\u001b[39;49;00m, \u001b[04m\u001b[32mStringType\u001b[39;49;00m, \u001b[34mtrue\u001b[39;49;00m)\r\n",
      "    ))\r\n",
      "      \r\n",
      "    \u001b[34mval\u001b[39;49;00m dataset \u001b[34m=\u001b[39;49;00m spark.read.option(\u001b[33m\"sep\"\u001b[39;49;00m, \u001b[33m\"\\t\"\u001b[39;49;00m)\r\n",
      "                            .option(\u001b[33m\"header\"\u001b[39;49;00m, \u001b[33m\"true\"\u001b[39;49;00m)\r\n",
      "                            .option(\u001b[33m\"quote\"\u001b[39;49;00m, \u001b[33m\"\"\u001b[39;49;00m)\r\n",
      "                            .schema(schema)\r\n",
      "                            .csv(s3InputData)\r\n",
      "\r\n",
      "    \u001b[37m// define analyzers that compute metrics\u001b[39;49;00m\r\n",
      "    \u001b[34mval\u001b[39;49;00m analysisResult\u001b[34m:\u001b[39;49;00m \u001b[36mAnalyzerContext\u001b[39;49;00m = { \u001b[04m\u001b[32mAnalysisRunner\u001b[39;49;00m\r\n",
      "          .onData(dataset)\r\n",
      "          .addAnalyzer(\u001b[04m\u001b[32mSize\u001b[39;49;00m())\r\n",
      "          .addAnalyzer(\u001b[04m\u001b[32mCompleteness\u001b[39;49;00m(\u001b[33m\"review_id\"\u001b[39;49;00m))\r\n",
      "          .addAnalyzer(\u001b[04m\u001b[32mApproxCountDistinct\u001b[39;49;00m(\u001b[33m\"review_id\"\u001b[39;49;00m))\r\n",
      "          .addAnalyzer(\u001b[04m\u001b[32mMean\u001b[39;49;00m(\u001b[33m\"star_rating\"\u001b[39;49;00m))\r\n",
      "          .addAnalyzer(\u001b[04m\u001b[32mCompliance\u001b[39;49;00m(\u001b[33m\"top star_rating\"\u001b[39;49;00m, \u001b[33m\"star_rating >= 4.0\"\u001b[39;49;00m))\r\n",
      "          .addAnalyzer(\u001b[04m\u001b[32mCorrelation\u001b[39;49;00m(\u001b[33m\"total_votes\"\u001b[39;49;00m, \u001b[33m\"star_rating\"\u001b[39;49;00m))\r\n",
      "          .addAnalyzer(\u001b[04m\u001b[32mCorrelation\u001b[39;49;00m(\u001b[33m\"total_votes\"\u001b[39;49;00m, \u001b[33m\"helpful_votes\"\u001b[39;49;00m))\r\n",
      "          \u001b[37m// compute metrics\u001b[39;49;00m\r\n",
      "          .run()\r\n",
      "        }\r\n",
      "\r\n",
      "    \u001b[37m// retrieve successfully computed metrics as a Spark data frame\u001b[39;49;00m\r\n",
      "    \u001b[34mval\u001b[39;49;00m metrics \u001b[34m=\u001b[39;49;00m successMetricsAsDataFrame(spark, analysisResult)\r\n",
      "    metrics.show(truncate\u001b[34m=\u001b[39;49;00m\u001b[34mfalse\u001b[39;49;00m)\r\n",
      "    metrics\r\n",
      "      .repartition(\u001b[34m1\u001b[39;49;00m)\r\n",
      "      .write\r\n",
      "      .mode(\u001b[04m\u001b[32mSaveMode\u001b[39;49;00m.\u001b[04m\u001b[32mOverwrite\u001b[39;49;00m)\r\n",
      "      .option(\u001b[33m\"header\"\u001b[39;49;00m, \u001b[34mtrue\u001b[39;49;00m)      \r\n",
      "      .option(\u001b[33m\"delimiter\"\u001b[39;49;00m, \u001b[33m\"\\t\"\u001b[39;49;00m)\r\n",
      "      .csv(\u001b[33ms\"\u001b[39;49;00m\u001b[33m${\u001b[39;49;00ms3OutputAnalyzeData\u001b[33m}\u001b[39;49;00m\u001b[33m/dataset-metrics\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m)\r\n",
      "\r\n",
      "    \u001b[37m// define data quality checks,\u001b[39;49;00m\r\n",
      "    \u001b[37m// compute metrics \u001b[39;49;00m\r\n",
      "    \u001b[37m// verify check conditions\u001b[39;49;00m\r\n",
      "    \u001b[34mval\u001b[39;49;00m verificationResult\u001b[34m:\u001b[39;49;00m \u001b[36mVerificationResult\u001b[39;49;00m = { \u001b[04m\u001b[32mVerificationSuite\u001b[39;49;00m()\r\n",
      "          \u001b[37m// data to run the verification on\u001b[39;49;00m\r\n",
      "          .onData(dataset)\r\n",
      "          .addCheck(\r\n",
      "            \u001b[04m\u001b[32mCheck\u001b[39;49;00m(\u001b[04m\u001b[32mCheckLevel\u001b[39;49;00m.\u001b[04m\u001b[32mError\u001b[39;49;00m, \u001b[33m\"Review Check\"\u001b[39;49;00m) \r\n",
      "              .hasSize(\u001b[34m_\u001b[39;49;00m >= \u001b[34m200000\u001b[39;49;00m) \u001b[37m// at least 200.000 rows\u001b[39;49;00m\r\n",
      "              .hasMin(\u001b[33m\"star_rating\"\u001b[39;49;00m, \u001b[34m_\u001b[39;49;00m == \u001b[34m1.0\u001b[39;49;00m) \u001b[37m// min is 1.0\u001b[39;49;00m\r\n",
      "              .hasMax(\u001b[33m\"star_rating\"\u001b[39;49;00m, \u001b[34m_\u001b[39;49;00m == \u001b[34m5.0\u001b[39;49;00m) \u001b[37m// max is 5.0\u001b[39;49;00m\r\n",
      "              .isComplete(\u001b[33m\"review_id\"\u001b[39;49;00m) \u001b[37m// should never be NULL\u001b[39;49;00m\r\n",
      "              .isUnique(\u001b[33m\"review_id\"\u001b[39;49;00m) \u001b[37m// should not contain duplicates\u001b[39;49;00m\r\n",
      "              .isComplete(\u001b[33m\"marketplace\"\u001b[39;49;00m) \u001b[37m// should never be NULL\u001b[39;49;00m\r\n",
      "              .isContainedIn(\u001b[33m\"marketplace\"\u001b[39;49;00m, \u001b[04m\u001b[32mArray\u001b[39;49;00m(\u001b[33m\"US\"\u001b[39;49;00m, \u001b[33m\"UK\"\u001b[39;49;00m, \u001b[33m\"DE\"\u001b[39;49;00m, \u001b[33m\"JP\"\u001b[39;49;00m, \u001b[33m\"FR\"\u001b[39;49;00m)) \r\n",
      "              )\r\n",
      "          .run()\r\n",
      "    }\r\n",
      "\r\n",
      "    \u001b[37m// convert check results to a Spark data frame\u001b[39;49;00m\r\n",
      "    \u001b[34mval\u001b[39;49;00m resultsDataFrame \u001b[34m=\u001b[39;49;00m checkResultsAsDataFrame(spark, verificationResult)\r\n",
      "    resultsDataFrame.show(truncate\u001b[34m=\u001b[39;49;00m\u001b[34mfalse\u001b[39;49;00m)\r\n",
      "    resultsDataFrame\r\n",
      "      .repartition(\u001b[34m1\u001b[39;49;00m)\r\n",
      "      .write\r\n",
      "      .mode(\u001b[04m\u001b[32mSaveMode\u001b[39;49;00m.\u001b[04m\u001b[32mOverwrite\u001b[39;49;00m)\r\n",
      "      .option(\u001b[33m\"header\"\u001b[39;49;00m, \u001b[34mtrue\u001b[39;49;00m)\r\n",
      "      .option(\u001b[33m\"delimiter\"\u001b[39;49;00m, \u001b[33m\"\\t\"\u001b[39;49;00m)\r\n",
      "      .csv(\u001b[33ms\"\u001b[39;49;00m\u001b[33m${\u001b[39;49;00ms3OutputAnalyzeData\u001b[33m}\u001b[39;49;00m\u001b[33m/constraint-checks\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m)\r\n",
      "    \r\n",
      "    \u001b[37m// generate the success metrics as a dataframe\u001b[39;49;00m\r\n",
      "    \u001b[34mval\u001b[39;49;00m verificationSuccessMetricsDataFrame \u001b[34m=\u001b[39;49;00m \u001b[04m\u001b[32mVerificationResult\u001b[39;49;00m\r\n",
      "      .successMetricsAsDataFrame(spark, verificationResult)\r\n",
      "\r\n",
      "    verificationSuccessMetricsDataFrame.show(truncate\u001b[34m=\u001b[39;49;00m\u001b[34mfalse\u001b[39;49;00m)\r\n",
      "    verificationSuccessMetricsDataFrame\r\n",
      "      .repartition(\u001b[34m1\u001b[39;49;00m)\r\n",
      "      .write\r\n",
      "      .mode(\u001b[04m\u001b[32mSaveMode\u001b[39;49;00m.\u001b[04m\u001b[32mOverwrite\u001b[39;49;00m)\r\n",
      "      .option(\u001b[33m\"header\"\u001b[39;49;00m, \u001b[34mtrue\u001b[39;49;00m)\r\n",
      "      .option(\u001b[33m\"delimiter\"\u001b[39;49;00m, \u001b[33m\"\\t\"\u001b[39;49;00m)\r\n",
      "      .csv(\u001b[33ms\"\u001b[39;49;00m\u001b[33m${\u001b[39;49;00ms3OutputAnalyzeData\u001b[33m}\u001b[39;49;00m\u001b[33m/success-metrics\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m)      \r\n",
      "\r\n",
      "    \u001b[37m// We ask deequ to compute constraint suggestions for us on the data\u001b[39;49;00m\r\n",
      "    \u001b[37m// using a default set of rules for constraint suggestion\u001b[39;49;00m\r\n",
      "    \u001b[34mval\u001b[39;49;00m suggestionsResult \u001b[34m=\u001b[39;49;00m { \u001b[04m\u001b[32mConstraintSuggestionRunner\u001b[39;49;00m()\r\n",
      "          .onData(dataset)\r\n",
      "          .addConstraintRules(\u001b[04m\u001b[32mRules\u001b[39;49;00m.\u001b[04m\u001b[32mDEFAULT\u001b[39;49;00m)\r\n",
      "          .run()\r\n",
      "    }\r\n",
      "\r\n",
      "    \u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36mspark.implicits._\u001b[39;49;00m \u001b[37m// for toDS method below\u001b[39;49;00m\r\n",
      "\r\n",
      "    \u001b[37m// We can now investigate the constraints that Deequ suggested. \u001b[39;49;00m\r\n",
      "    \u001b[34mval\u001b[39;49;00m suggestionsDataFrame \u001b[34m=\u001b[39;49;00m suggestionsResult.constraintSuggestions.flatMap { \r\n",
      "          \u001b[34mcase\u001b[39;49;00m (column, suggestions) \u001b[34m=>\u001b[39;49;00m \r\n",
      "            suggestions.map { constraint \u001b[34m=>\u001b[39;49;00m\r\n",
      "              (column, constraint.description, constraint.codeForConstraint)\r\n",
      "            } \r\n",
      "    }.toSeq.toDS()\r\n",
      "      \r\n",
      "    suggestionsDataFrame.show(truncate\u001b[34m=\u001b[39;49;00m\u001b[34mfalse\u001b[39;49;00m)\r\n",
      "    suggestionsDataFrame\r\n",
      "      .repartition(\u001b[34m1\u001b[39;49;00m)      \r\n",
      "      .write      \r\n",
      "      .mode(\u001b[04m\u001b[32mSaveMode\u001b[39;49;00m.\u001b[04m\u001b[32mOverwrite\u001b[39;49;00m)\r\n",
      "      .option(\u001b[33m\"header\"\u001b[39;49;00m, \u001b[34mtrue\u001b[39;49;00m)  \r\n",
      "      .option(\u001b[33m\"delimiter\"\u001b[39;49;00m, \u001b[33m\"\\t\"\u001b[39;49;00m)\r\n",
      "      .csv(\u001b[33ms\"\u001b[39;49;00m\u001b[33m${\u001b[39;49;00ms3OutputAnalyzeData\u001b[33m}\u001b[39;49;00m\u001b[33m/constraint-suggestions\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m)      \r\n",
      "  }\r\n",
      "}\r\n"
     ]
    }
   ],
   "source": [
    "!pygmentize deequ/preprocess-deequ.scala"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.processing import ScriptProcessor\n",
    "\n",
    "processor = ScriptProcessor(base_job_name='spark-amazon-reviews-analyzer',\n",
    "                            image_uri=image_uri,\n",
    "                            command=['/opt/program/submit'],\n",
    "                            role=role,\n",
    "                            instance_count=2, # instance_count needs to be > 1 or you will see the following error:  \"INFO yarn.Client: Application report for application_ (state: ACCEPTED)\"\n",
    "                            instance_type='ml.r5.2xlarge',\n",
    "                            env={\n",
    "                                'mode': 'jar',\n",
    "                                'main_class': 'Main'\n",
    "                            })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "s3://sagemaker-us-west-2-032934710550/amazon-reviews-pds/tsv/\n"
     ]
    }
   ],
   "source": [
    "s3_input_data = 's3://{}/amazon-reviews-pds/tsv/'.format(bucket)\n",
    "print(s3_input_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020-08-22 17:44:30   18997559 amazon_reviews_us_Digital_Software_v1_00.tsv.gz\r\n",
      "2020-08-22 17:44:34   27442648 amazon_reviews_us_Digital_Video_Games_v1_00.tsv.gz\r\n"
     ]
    }
   ],
   "source": [
    "!aws s3 ls $s3_input_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup Output Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing job name:  amazon-reviews-spark-analyzer-2020-08-22-19-16-22\n"
     ]
    }
   ],
   "source": [
    "from time import gmtime, strftime\n",
    "timestamp_prefix = strftime(\"%Y-%m-%d-%H-%M-%S\", gmtime())\n",
    "\n",
    "output_prefix = 'amazon-reviews-spark-analyzer-{}'.format(timestamp_prefix)\n",
    "processing_job_name = 'amazon-reviews-spark-analyzer-{}'.format(timestamp_prefix)\n",
    "\n",
    "print('Processing job name:  {}'.format(processing_job_name))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "s3://sagemaker-us-west-2-032934710550/amazon-reviews-spark-analyzer-2020-08-22-19-16-22/output\n"
     ]
    }
   ],
   "source": [
    "s3_output_analyze_data = 's3://{}/{}/output'.format(bucket, output_prefix)\n",
    "\n",
    "print(s3_output_analyze_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Start the Spark Processing Job\n",
    "\n",
    "_Notes on Invoking from Lambda:_\n",
    "* However, if we use the boto3 SDK (ie. with a Lambda), we need to copy the `preprocess.py` file to S3 and specify the everything include --py-files, etc.\n",
    "* We would need to do the following before invoking the Lambda:\n",
    "     !aws s3 cp preprocess.py s3://<location>/sagemaker/spark-preprocess-reviews-demo/code/preprocess.py\n",
    "     !aws s3 cp preprocess.py s3://<location>/sagemaker/spark-preprocess-reviews-demo/py_files/preprocess.py\n",
    "* Then reference the s3://<location> above in the --py-files, etc.\n",
    "* See Lambda example code in this same project for more details.\n",
    "\n",
    "_Notes on not using ProcessingInput and Output:_\n",
    "* Since Spark natively reads/writes from/to S3 using s3a://, we can avoid the copy required by ProcessingInput and ProcessingOutput (FullyReplicated or ShardedByS3Key) and just specify the S3 input and output buckets/prefixes._\"\n",
    "* See https://github.com/awslabs/amazon-sagemaker-examples/issues/994 for issues related to using /opt/ml/processing/input/ and output/\n",
    "* If we use ProcessingInput, the data will be copied to each node (which we don't want in this case since Spark already handles this)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Parameter 'session' will be renamed to 'sagemaker_session' in SageMaker Python SDK v2.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Job Name:  spark-amazon-reviews-analyzer-2020-08-22-19-16-22-357\n",
      "Inputs:  [{'InputName': 'code', 'S3Input': {'S3Uri': 's3://sagemaker-us-west-2-032934710550/spark-amazon-reviews-analyzer-2020-08-22-19-16-22-357/input/code/preprocess-deequ.py', 'LocalPath': '/opt/ml/processing/input/code', 'S3DataType': 'S3Prefix', 'S3InputMode': 'File', 'S3DataDistributionType': 'FullyReplicated', 'S3CompressionType': 'None'}}]\n",
      "Outputs:  [{'OutputName': 'null-output', 'S3Output': {'S3Uri': 's3://sagemaker-us-west-2-032934710550/spark-amazon-reviews-analyzer-2020-08-22-19-16-22-357/output/null-output', 'LocalPath': '/opt/ml/processing/output', 'S3UploadMode': 'EndOfJob'}}]\n"
     ]
    }
   ],
   "source": [
    "from sagemaker.processing import ProcessingOutput\n",
    "\n",
    "processor.run(code='preprocess-deequ.py',\n",
    "              arguments=['s3_input_data', s3_input_data,\n",
    "                         's3_output_analyze_data', s3_output_analyze_data,\n",
    "              ],\n",
    "              # See https://github.com/aws/sagemaker-python-sdk/issues/1341 \n",
    "              #   for why we need to specify a null-output\n",
    "              outputs=[\n",
    "                  ProcessingOutput(s3_upload_mode='EndOfJob',\n",
    "                                   output_name='null-output',\n",
    "                                   source='/opt/ml/processing/output')\n",
    "              ],\n",
    "              logs=True,\n",
    "              wait=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<b>Review <a target=\"blank\" href=\"https://console.aws.amazon.com/cloudwatch/home?region=us-west-2#logStream:group=/aws/sagemaker/ProcessingJobs;prefix=spark-amazon-reviews-analyzer-2020-08-22-19-16-22-357;streamFilter=typeLogStreamPrefix\">CloudWatch Logs</a> After About 5 Minutes</b>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.core.display import display, HTML\n",
    "\n",
    "processing_job_name = processor.jobs[-1].describe()['ProcessingJobName']\n",
    "\n",
    "display(HTML('<b>Review <a target=\"blank\" href=\"https://console.aws.amazon.com/cloudwatch/home?region={}#logStream:group=/aws/sagemaker/ProcessingJobs;prefix={};streamFilter=typeLogStreamPrefix\">CloudWatch Logs</a> After About 5 Minutes</b>'.format(region, processing_job_name)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<b>Review <a target=\"blank\" href=\"https://s3.console.aws.amazon.com/s3/buckets/sagemaker-us-west-2-032934710550/amazon-reviews-spark-analyzer-2020-08-22-19-16-22/?region=us-west-2&tab=overview\">S3 Output Data</a> After The Spark Job Has Completed</b>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.core.display import display, HTML\n",
    "\n",
    "s3_job_output_prefix = output_prefix\n",
    "\n",
    "display(HTML('<b>Review <a target=\"blank\" href=\"https://s3.console.aws.amazon.com/s3/buckets/{}/{}/?region={}&tab=overview\">S3 Output Data</a> After The Spark Job Has Completed</b>'.format(bucket, s3_job_output_prefix, region)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Please Wait Until the Processing Job Completes!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "InProgress\n",
      "\n",
      "\n",
      "{'ProcessingInputs': [{'InputName': 'code', 'S3Input': {'S3Uri': 's3://sagemaker-us-west-2-032934710550/spark-amazon-reviews-analyzer-2020-08-22-19-16-22-357/input/code/preprocess-deequ.py', 'LocalPath': '/opt/ml/processing/input/code', 'S3DataType': 'S3Prefix', 'S3InputMode': 'File', 'S3DataDistributionType': 'FullyReplicated', 'S3CompressionType': 'None'}}], 'ProcessingOutputConfig': {'Outputs': [{'OutputName': 'null-output', 'S3Output': {'S3Uri': 's3://sagemaker-us-west-2-032934710550/spark-amazon-reviews-analyzer-2020-08-22-19-16-22-357/output/null-output', 'LocalPath': '/opt/ml/processing/output', 'S3UploadMode': 'EndOfJob'}}]}, 'ProcessingJobName': 'spark-amazon-reviews-analyzer-2020-08-22-19-16-22-357', 'ProcessingResources': {'ClusterConfig': {'InstanceCount': 2, 'InstanceType': 'ml.r5.2xlarge', 'VolumeSizeInGB': 30}}, 'StoppingCondition': {'MaxRuntimeInSeconds': 86400}, 'AppSpecification': {'ImageUri': '032934710550.dkr.ecr.us-west-2.amazonaws.com/amazon-reviews-spark-analyzer:latest', 'ContainerEntrypoint': ['/opt/program/submit', '/opt/ml/processing/input/code/preprocess-deequ.py'], 'ContainerArguments': ['s3_input_data', 's3://sagemaker-us-west-2-032934710550/amazon-reviews-pds/tsv/', 's3_output_analyze_data', 's3://sagemaker-us-west-2-032934710550/amazon-reviews-spark-analyzer-2020-08-22-19-16-22/output']}, 'Environment': {'main_class': 'Main', 'mode': 'jar'}, 'RoleArn': 'arn:aws:iam::032934710550:role/TeamRole', 'ProcessingJobArn': 'arn:aws:sagemaker:us-west-2:032934710550:processing-job/spark-amazon-reviews-analyzer-2020-08-22-19-16-22-357', 'ProcessingJobStatus': 'InProgress', 'LastModifiedTime': datetime.datetime(2020, 8, 22, 19, 16, 22, 754000, tzinfo=tzlocal()), 'CreationTime': datetime.datetime(2020, 8, 22, 19, 16, 22, 754000, tzinfo=tzlocal()), 'ResponseMetadata': {'RequestId': 'f3fd895d-ecd2-4536-ab9e-1978f1e05515', 'HTTPStatusCode': 200, 'HTTPHeaders': {'x-amzn-requestid': 'f3fd895d-ecd2-4536-ab9e-1978f1e05515', 'content-type': 'application/x-amz-json-1.1', 'content-length': '1626', 'date': 'Sat, 22 Aug 2020 19:16:22 GMT'}, 'RetryAttempts': 0}}\n"
     ]
    }
   ],
   "source": [
    "running_processor = sagemaker.processing.ProcessingJob.from_processing_name(processing_job_name=processing_job_name,\n",
    "                                                                            sagemaker_session=sagemaker_session)\n",
    "\n",
    "processing_job_description = running_processor.describe()\n",
    "\n",
    "processing_job_status = processing_job_description['ProcessingJobStatus']\n",
    "print('\\n')\n",
    "print(processing_job_status)\n",
    "print('\\n')\n",
    "\n",
    "print(processing_job_description)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# _Please Wait Until the ^^ Processing Job ^^ Completes Above._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "...............\n",
      ".."
     ]
    },
    {
     "ename": "UnexpectedStatusException",
     "evalue": "Error for Processing job spark-amazon-reviews-analyzer-2020-08-22-19-16-22-357: Failed. Reason: ClientError: API error (404): manifest for 032934710550.dkr.ecr.us-west-2.amazonaws.com/amazon-reviews-spark-analyzer:latest not found: manifest unknown: Requested image not found",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mUnexpectedStatusException\u001b[0m                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-22-95ceb41b6273>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mrunning_processor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/anaconda3/envs/python3/lib/python3.6/site-packages/sagemaker/processing.py\u001b[0m in \u001b[0;36mwait\u001b[0;34m(self, logs)\u001b[0m\n\u001b[1;32m    729\u001b[0m         \"\"\"\n\u001b[1;32m    730\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlogs\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 731\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msagemaker_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlogs_for_processing_job\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjob_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwait\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    732\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    733\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msagemaker_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwait_for_processing_job\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjob_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/python3/lib/python3.6/site-packages/sagemaker/session.py\u001b[0m in \u001b[0;36mlogs_for_processing_job\u001b[0;34m(self, job_name, wait, poll)\u001b[0m\n\u001b[1;32m   3165\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3166\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mwait\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3167\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_check_job_status\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjob_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdescription\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"ProcessingJobStatus\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3168\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mdot\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3169\u001b[0m                 \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/python3/lib/python3.6/site-packages/sagemaker/session.py\u001b[0m in \u001b[0;36m_check_job_status\u001b[0;34m(self, job, desc, status_key_name)\u001b[0m\n\u001b[1;32m   2669\u001b[0m                 ),\n\u001b[1;32m   2670\u001b[0m                 \u001b[0mallowed_statuses\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"Completed\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"Stopped\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2671\u001b[0;31m                 \u001b[0mactual_status\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstatus\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2672\u001b[0m             )\n\u001b[1;32m   2673\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mUnexpectedStatusException\u001b[0m: Error for Processing job spark-amazon-reviews-analyzer-2020-08-22-19-16-22-357: Failed. Reason: ClientError: API error (404): manifest for 032934710550.dkr.ecr.us-west-2.amazonaws.com/amazon-reviews-spark-analyzer:latest not found: manifest unknown: Requested image not found"
     ]
    }
   ],
   "source": [
    "running_processor.wait()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inspect the Processed Output \n",
    "\n",
    "## These are the quality checks on our dataset.\n",
    "\n",
    "## _The next cells will not work properly until the job completes above._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!aws s3 ls --recursive $s3_output_analyze_data/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Copy the Output from S3 to Local\n",
    "* dataset-metrics/\n",
    "* constraint-checks/\n",
    "* success-metrics/\n",
    "* constraint-suggestions/\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "!aws s3 cp --recursive $s3_output_analyze_data ./amazon-reviews-spark-analyzer/ --exclude=\"*\" --include=\"*.csv\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analyze Constraint Checks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "def load_dataset(path, sep, header):\n",
    "    data = pd.concat([pd.read_csv(f, sep=sep, header=header) for f in glob.glob('{}/*.csv'.format(path))], ignore_index = True)\n",
    "\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_constraint_checks = load_dataset(path='./amazon-reviews-spark-analyzer/constraint-checks/', sep='\\t', header=0)\n",
    "df_constraint_checks[['check', 'constraint', 'constraint_status', 'constraint_message']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analyze Dataset Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_dataset_metrics = load_dataset(path='./amazon-reviews-spark-analyzer/dataset-metrics/', sep='\\t', header=0)\n",
    "df_dataset_metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analyze Success Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_success_metrics = load_dataset(path='./amazon-reviews-spark-analyzer/success-metrics/', sep='\\t', header=0)\n",
    "df_success_metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analyze Constraint Suggestions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_constraint_suggestions = load_dataset(path='./amazon-reviews-spark-analyzer/constraint-suggestions/', sep='\\t', header=0)\n",
    "df_constraint_suggestions.columns=['column_name', 'description', 'code']\n",
    "df_constraint_suggestions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Save for the Next Notebook(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%store df_dataset_metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%javascript\n",
    "Jupyter.notebook.save_checkpoint();\n",
    "Jupyter.notebook.session.delete();"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
